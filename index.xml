<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Unique Divine</title>
    <link>https://unique-divine.github.io/</link>
    <description>Recent content on Unique Divine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright ¬© 2020</copyright>
    <lastBuildDate>Sun, 15 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://unique-divine.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GANs for Genomics</title>
      <link>https://unique-divine.github.io/projects/gans-for-genomics/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/projects/gans-for-genomics/</guid>
      <description>Motivation Multiple papers have shown that neural networks are generally less effective for prediction of complex genetic disorders than polygenic risk scores (PRSs) even though PRSs are linear predictors [Pinto et al., 2019] [Mamani, 2020] [Badr√© et al., 2020]. I found this surprising at first but grew to understand why after discovering what challenges arrise when working with genomics datasets for predictive modeling.
 High-dimensionality feature sets: Input vectors can have on the order of 100,000+ features.</description>
    </item>
    
    <item>
      <title>On Reflection &amp; Logging „ÄåÈÅéÂéª„ÇíÈ°ß„Åø„Çã„Åì„Å®„Äç</title>
      <link>https://unique-divine.github.io/post/poh/reflection/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/post/poh/reflection/</guid>
      <description>I like to dream as much as anyone. However, I&amp;rsquo;m also a huge fan of results and tangible evidence.
I was an athlete for probably 10 years of my life and so I&amp;rsquo;d tried a multitude of different strength and conditioning programs, diets, and exercise methods. There was a point in time where I was doing a more intense phase of bodybuilding from the tail end of high school leading into my first few years of college.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://unique-divine.github.io/about/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/about/</guid>
      <description>I&amp;rsquo;m a programmer that makes music and writes. I&amp;rsquo;m obsessed with learning, solving interesting problems, and creating stuff.
What I&amp;rsquo;m currently focused on: ¬ß1. Work  üìä Data science @ IBM  ¬ß2. Side projects  üíª A research project in vision-based, deep reinforcement learning [code] üß¨ Computational genomics research | I leverage deep learning to generate and learn from synthetic DNA samples. [code] ü§ñ Artificial intelligence and DevOps work on healthcare applications @ Applied Technology Solutions, Inc.</description>
    </item>
    
    <item>
      <title>Why&#39;d I learn Japanese again?</title>
      <link>https://unique-divine.github.io/post/japanese/why_learn_jp_again/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/post/japanese/why_learn_jp_again/</guid>
      <description>I recently crossed the 10,000 sentence mark in my dearly beloved spaced repitition software (SRS), Anki, after noticing that my deck includes 10,317 mature sentence cards.
At the onset of my learning journey, I was excited and inspired by Khatzumoto&amp;rsquo;s progress and figured I could learn a thing or two by temporarily giving in to dogma.
This milestone took me roughly two years of consistent grinding, where I averaged between 60-200 minutes of daily study time depending on the month and my availability.</description>
    </item>
    
    <item>
      <title>Neural Networks for Gravitational Lens Modeling</title>
      <link>https://unique-divine.github.io/projects/nn-grav_lens/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/projects/nn-grav_lens/</guid>
      <description>Neural Networks for Gravitational Lens Modeling In this project, I use convolutional neural networks (with TensorFlow) to estimate the parameters of strong gravitational lenses, namely the coordinates of the center of the lens in two dimensions, the Einstein radius, and the complex ellipticities.
These parameters are important because they enable us to quantify the distortions of images due to gravitational lensing. The fact that the deflection of light due to gravitational lensing is dependent on the mass structure of the lens means that recovering these parameters effectively allows us to measure and characterize the distribution of mass.</description>
    </item>
    
    <item>
      <title>Click-Through Rate Prediction with Stochastic Gradient Descent</title>
      <link>https://unique-divine.github.io/projects/ctr-prediction/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/projects/ctr-prediction/</guid>
      <description>Motivation: Click-through rate (CTR) is the ratio of users who click on a link to the number of users who see it. It is commonly used to measure the success of marketing campaigns.
The goal of this project is to predict whether advertisements will be clicked or not. In solving this problem, I implement a stochastic gradient descent algorithm called Pegasos, which has proven to be effective for CTR predictions in the past (Sculley et al.</description>
    </item>
    
    <item>
      <title>Banknote Fraud Detection</title>
      <link>https://unique-divine.github.io/projects/banknote_fraud/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/projects/banknote_fraud/</guid>
      <description>Banknote Fraud Detection - Decision Trees from Scratch The goal of this project was to develop accurate predictive models to solve a binary classification problem: detecting fraudulent banknotes. I figured that learning to write decision tree algorithms from scratch could serve as an effective technical exercise. Unexpectedly, this from-scratch implementation ended up outperforming the default trees from Scikit-learn.
If you&amp;rsquo;d like to view the notebook without downloading anything, go here:   Data Description: The data was downloaded from this source.</description>
    </item>
    
    <item>
      <title>Breaking Into Data Science I: Plan of Attack</title>
      <link>https://unique-divine.github.io/post/ai/breaking-into-ds-i/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://unique-divine.github.io/post/ai/breaking-into-ds-i/</guid>
      <description>In 5th grade, one of my classmates fell down and broke his arm during a game of two-hand touch football, so the elementary school banned the sport during recess. I was pretty upset about this. Lots of kids were. This was Texas, after all, and we loved our football.
I experimented with other recess activities: hopscotch, double Dutch, chalk drawing, tetherball, basketball, dodgeball, kickball. Many of these activities just didn&amp;rsquo;t do it for me.</description>
    </item>
    
  </channel>
</rss>